WEBVTT

1
00:00:08.965 --> 00:00:12.550
We use statistics in a lot of
different ways in data science.

2
00:00:12.550 --> 00:00:17.019
In this lecture, I want to refresh
your knowledge of hypothesis testing,

3
00:00:17.019 --> 00:00:20.995
which is a core data analysis
activity behind experimentation.

4
00:00:20.995 --> 00:00:25.157
We're starting to see experimentation
used more and more commonly

5
00:00:25.157 --> 00:00:29.838
outside of academic scientific, and
in day to day business environments.

6
00:00:29.838 --> 00:00:34.510
Part of the reason for this is
the rise of big data and web commerce.

7
00:00:34.510 --> 00:00:37.300
It's now easy to change
your digital storefront and

8
00:00:37.300 --> 00:00:40.620
deliver a different experience
to some of your customers, and

9
00:00:40.620 --> 00:00:44.560
then see how those customer actions
might differ from one another.

10
00:00:44.560 --> 00:00:49.298
For instance, if you sell books, you might
want to have one condition where the cover

11
00:00:49.298 --> 00:00:51.565
of the book is featured prominently
on the web page and

12
00:00:51.565 --> 00:00:55.984
another condition where the focus is on
the author and the reviews of the book.

13
00:00:55.984 --> 00:00:58.382
This is often called A/B testing.

14
00:00:58.382 --> 00:01:03.246
And while it's not unique to this time in
history, it's now becoming so common that

15
00:01:03.246 --> 00:01:07.918
if you're using a website, you are
undoubtedly part of an A/B test somewhere.

16
00:01:07.918 --> 00:01:12.378
This raises some interesting ethical
questions and I've added a reading to

17
00:01:12.378 --> 00:01:16.558
the course resources and I would like
to encourage you to check it out and

18
00:01:16.558 --> 00:01:20.410
join in on the discussion, but
let's refocus back on statistics.

19
00:01:21.470 --> 00:01:24.347
A hypothesis is a statement
that we can test.

20
00:01:24.347 --> 00:01:28.542
I'll pull an example from my own
research area of educational technology and

21
00:01:28.542 --> 00:01:29.858
learning analytics.

22
00:01:29.858 --> 00:01:34.567
Let's say that we have an expectation that
when a new course is launched on a MOOC

23
00:01:34.567 --> 00:01:38.739
platform, the keenest students find
out about it and all flock to it.

24
00:01:38.739 --> 00:01:42.507
Thus, we might expect that those
students who sign up quite quickly after

25
00:01:42.507 --> 00:01:45.487
the course is launched will have
higher performance than those

26
00:01:45.487 --> 00:01:48.850
students who signed up after the MOOC
has been around for a while.

27
00:01:48.850 --> 00:01:53.469
In this example, we have samples from two
different groups which we want to compare.

28
00:01:53.469 --> 00:01:55.740
The early sign ups and the late sign ups.

29
00:01:56.940 --> 00:02:02.980
When we do hypothesis testing, we hold out
that our hypothesis as the alternative and

30
00:02:02.980 --> 00:02:07.010
we create a second hypothesis
called the null hypothesis,

31
00:02:07.010 --> 00:02:11.100
which in this case would be that there
is no difference between groups.

32
00:02:11.100 --> 00:02:14.820
We then examine the groups to determine
whether this null hypothesis is

33
00:02:14.820 --> 00:02:15.430
true or not.

34
00:02:16.560 --> 00:02:19.371
If we find that there is
a difference between groups,

35
00:02:19.371 --> 00:02:23.064
then we can reject the null hypothesis and
we accept our alternative.

36
00:02:23.064 --> 00:02:25.035
There are subtleties in this description.

37
00:02:25.035 --> 00:02:28.553
We aren't saying that our
hypothesis is true per se, but

38
00:02:28.553 --> 00:02:32.822
we're saying that there's evidence
against the null hypothesis.

39
00:02:32.822 --> 00:02:37.396
So, we're more confident in
our alternative hypothesis.

40
00:02:37.396 --> 00:02:39.685
Let's see an example of this.

41
00:02:39.685 --> 00:02:42.885
We can load a file called grades.csv.

42
00:02:42.885 --> 00:02:45.008
If we take a look at
the DataFrame inside,

43
00:02:45.008 --> 00:02:47.203
we see we have six different assignments,

44
00:02:47.203 --> 00:02:49.258
each with a submission time.

45
00:02:49.258 --> 00:02:53.853
And it looks like there are just under
3,000 entries in this data file.

46
00:02:53.853 --> 00:02:58.470
For the purpose of this lecture, let's
segment this population into two pieces.

47
00:02:58.470 --> 00:03:01.650
Those who finish the first assignment
by the end of December 2015 and

48
00:03:01.650 --> 00:03:04.340
those who finish it sometimes after that.

49
00:03:05.470 --> 00:03:08.120
I just made this date up and
it gives us two DataFrames,

50
00:03:08.120 --> 00:03:09.530
which are roughly the same size.

51
00:03:11.010 --> 00:03:14.770
As you've seen, the pandas DataFrame
object has a variety of statistical

52
00:03:14.770 --> 00:03:17.210
functions associated with it.

53
00:03:17.210 --> 00:03:19.860
If we call the mean function
directly on the DataFrame,

54
00:03:19.860 --> 00:03:22.579
we see that each of the means for
the assignments are calculated.

55
00:03:23.640 --> 00:03:28.488
Note that the datetime values are ignored
as panda's knows this isn't a number,

56
00:03:28.488 --> 00:03:29.692
but an object type.

57
00:03:29.692 --> 00:03:33.165
If we look at the mean values for
the late DataFrame as well,

58
00:03:33.165 --> 00:03:35.468
we get surprisingly similar numbers.

59
00:03:35.468 --> 00:03:37.180
There are slight differences, though.

60
00:03:37.180 --> 00:03:39.430
It looks like the end
of the six assignments,

61
00:03:39.430 --> 00:03:42.400
the early users are doing better
by about a percentage point.

62
00:03:43.400 --> 00:03:47.100
So, is this enough to go ahead and make
some interventions to actually try and

63
00:03:47.100 --> 00:03:50.180
change something in the way we teach?

64
00:03:50.180 --> 00:03:55.370
When doing hypothesis testing, we have to
choose a significance level as a threshold

65
00:03:55.370 --> 00:03:58.930
for how much of a chance
we're willing to accept.

66
00:03:58.930 --> 00:04:02.140
This significance level is
typically called alpha.

67
00:04:02.140 --> 00:04:05.890
It can vary greatly, depending on what
you're going to do with the result and

68
00:04:05.890 --> 00:04:07.850
the amount of noise you
expect in your data.

69
00:04:08.870 --> 00:04:13.221
For instance, in social sciences research,
a value of 0.05 or

70
00:04:13.221 --> 00:04:17.940
0.01 is often used,
which indicates a tolerance for

71
00:04:17.940 --> 00:04:22.350
a probability of between 5% and
1% of chance.

72
00:04:22.350 --> 00:04:25.490
In a physics experiment where the
conditions are much more controlled and

73
00:04:25.490 --> 00:04:29.320
thus, the burden of proof is much higher,
you might expect to see alpha

74
00:04:29.320 --> 00:04:33.590
levels of 10 to the negative 5 or
100,000th of a percentage.

75
00:04:35.490 --> 00:04:40.280
You can think of the significance level
from the perspective of interventions as

76
00:04:40.280 --> 00:04:44.165
well and this is something I run
into regularly with my research.

77
00:04:44.165 --> 00:04:48.637
What am I going to do when I find out that
two student populations are different?

78
00:04:48.637 --> 00:04:52.076
For instance, if I'm going to send
an email nudge to encourage students to

79
00:04:52.076 --> 00:04:56.720
continue working on their homework,
that's a pretty low-cost intervention.

80
00:04:56.720 --> 00:05:00.540
Emails are cheap and while I certainly
don't want to annoy students,

81
00:05:00.540 --> 00:05:03.140
one extra email isn't
going to ruin their day.

82
00:05:03.140 --> 00:05:06.101
But what if the intervention
is a little more involved,

83
00:05:06.101 --> 00:05:09.885
like having our tutorial assistant
followup with a student via phone?

84
00:05:09.885 --> 00:05:13.189
This is all of a sudden much more
expensive for both the institution and for

85
00:05:13.189 --> 00:05:13.834
the student.

86
00:05:13.834 --> 00:05:16.430
So, I might want to ensure
a higher burden of proof.

87
00:05:17.530 --> 00:05:19.187
So the threshold you set for

88
00:05:19.187 --> 00:05:22.868
alpha depends on what you might
do with the result, as well.

89
00:05:22.868 --> 00:05:29.181
For this example, let's use a threshold
of 0.05 for our alpha or 5%.

90
00:05:29.181 --> 00:05:33.693
Now, how do we actually test whether
these means are different in Python?

91
00:05:33.693 --> 00:05:37.782
The SciPy library contains a number
of different statistical tests and

92
00:05:37.782 --> 00:05:40.584
forms a basis for
hypothesis testing in Python.

93
00:05:40.584 --> 00:05:45.060
A t-test is one way to compare
the means of two different populations.

94
00:05:45.060 --> 00:05:48.444
In the SciPy library,
the ttest_ind function will

95
00:05:48.444 --> 00:05:53.210
compare two independent samples to
see if they have different means.

96
00:05:53.210 --> 00:05:56.721
I'm not going to go into the details
of any of this statistical tests here,

97
00:05:56.721 --> 00:06:01.247
but instead, we'd recommend that you check
out the Wikipedia page on particular test

98
00:06:01.247 --> 00:06:06.072
or consider taking a full statistics
course if this is unfamiliar to you.

99
00:06:06.072 --> 00:06:09.637
But I do want to note that most
statistical tests expect that

100
00:06:09.637 --> 00:06:13.071
the data conforms to a certain
distribution, a shape.

101
00:06:13.071 --> 00:06:19.098
So, you shouldn't apply such tests blindly
and should investigate your data first.

102
00:06:19.098 --> 00:06:21.649
If we want to compare
the assignment grades for

103
00:06:21.649 --> 00:06:26.012
the first assignment between the two
populations, we could generate a t-test

104
00:06:26.012 --> 00:06:29.182
by passing these two series
into the ttest_ind function.

105
00:06:29.182 --> 00:06:33.144
The result is a tuple with a test
statistic and a p-value.

106
00:06:33.144 --> 00:06:36.887
The p-value here is much
larger than our 0.05.

107
00:06:36.887 --> 00:06:39.591
So we cannot reject the null hypothesis,

108
00:06:39.591 --> 00:06:42.690
which is that the two
populations are the same.

109
00:06:42.690 --> 00:06:45.349
In more lay terms,
we would say that there's

110
00:06:45.349 --> 00:06:49.721
no statistically significant difference
between these two sample means.

111
00:06:49.721 --> 00:06:51.413
Let's check with assignment two grade.

112
00:06:58.037 --> 00:07:01.118
No, that's much larger than 0.05 too.

113
00:07:01.118 --> 00:07:02.639
How about with assignment three?

114
00:07:08.857 --> 00:07:13.191
Well, that's much closer, but
still beyond our threshold value.

115
00:07:13.191 --> 00:07:17.832
It's important to stop here and talk
about serious process problem with how we're

116
00:07:17.832 --> 00:07:22.557
handling this investigation of the
difference between these two populations.

117
00:07:22.557 --> 00:07:27.424
When we set the alpha to be 0.05,
we're saying that we expect it that there

118
00:07:27.424 --> 00:07:31.280
will be positive result,
5% of the time just due to chance.

119
00:07:32.440 --> 00:07:36.880
As we run more and more t-tests, we're
more likely to find a positive result

120
00:07:36.880 --> 00:07:39.250
just because of the number
of t-tests we have run.

121
00:07:40.340 --> 00:07:44.853
When a data scientist runs many tests
in this way, it's called p-hacking or

122
00:07:44.853 --> 00:07:48.039
dredging and
it's a serious methodological issue.

123
00:07:48.039 --> 00:07:54.875
P-hacking results in spurious correlations
instead of generalizable results.

124
00:07:54.875 --> 00:07:57.684
There are a couple of different
ways you can deal with p-hacking.

125
00:07:57.684 --> 00:08:00.490
The first is called
the Bonferroni correction.

126
00:08:00.490 --> 00:08:03.187
In this case,
you simply tighten your alpha value,

127
00:08:03.187 --> 00:08:07.187
the threshold of significance, based on
the number of tests you're running.

128
00:08:07.187 --> 00:08:12.130
So if you choose 0.05 with 1 test,
and you want to run 3 tests,

129
00:08:12.130 --> 00:08:19.406
you reduce alpha by multiplying 0.05 by
one-third to get a new value of 0.017.

130
00:08:19.406 --> 00:08:23.400
I personally find this approach
to be very conservative.

131
00:08:23.400 --> 00:08:26.639
Another option is to hold
out some of your data for

132
00:08:26.639 --> 00:08:29.799
testing to see how
generalizable your result is.

133
00:08:29.799 --> 00:08:32.550
In this case,
we might take half of our data for

134
00:08:32.550 --> 00:08:35.741
each of the two DataFrames,
run our t-test with that,

135
00:08:35.741 --> 00:08:39.435
form specific hypothesis based
on the result of these tests,

136
00:08:39.435 --> 00:08:42.500
then run very limited tests
on the rest of the data.

137
00:08:43.540 --> 00:08:46.400
This method is actually heavily
used in machine learning when

138
00:08:46.400 --> 00:08:50.410
building predictive models, where it's
called cross fold validation and

139
00:08:50.410 --> 00:08:53.740
you'll learn more about this in
third course in this specialization.

140
00:08:55.270 --> 00:08:59.240
A final method which has come about is
the pre-registration of your experiment.

141
00:09:00.400 --> 00:09:04.060
In this step, you would outline what
you expect to find and why, and

142
00:09:04.060 --> 00:09:08.120
describe the test that would
backup a positive proof of this.

143
00:09:08.120 --> 00:09:12.150
You register it with a third party, in
academic circles, this is often a journal

144
00:09:12.150 --> 00:09:14.330
who determines whether it's
a reasonable test to run or not.

145
00:09:15.360 --> 00:09:17.707
You then run your study and
report the results,

146
00:09:17.707 --> 00:09:20.299
regardless as to whether
they were positive or not.

147
00:09:20.299 --> 00:09:25.060
Here, there is a larger burden on
connecting to existing theory since

148
00:09:25.060 --> 00:09:29.661
you need to convince reviewers that
the experiment is likely to test

149
00:09:29.661 --> 00:09:31.531
fully a given hypothesis.

150
00:09:31.531 --> 00:09:32.611
In this lecture,

151
00:09:32.611 --> 00:09:37.147
we've discussed just some of the basics
of hypothesis testing in Python.

152
00:09:37.147 --> 00:09:41.421
I introduced you to the SciPy library,
which you can use for t-testing.

153
00:09:41.421 --> 00:09:44.384
We've discussed some of the practical
issues which arise from looking for

154
00:09:44.384 --> 00:09:46.420
statistical significance.

155
00:09:46.420 --> 00:09:49.240
There's much more to learn
about hypothesis testing.

156
00:09:49.240 --> 00:09:51.120
For instance,
there are different tests used,

157
00:09:51.120 --> 00:09:53.800
depending on the shape of your data, and
different ways to report

158
00:09:53.800 --> 00:09:57.610
results instead of just p-values,
such as confidence intervals.

159
00:09:57.610 --> 00:10:00.450
But I hope this gives you a start
to comparing the means of two

160
00:10:00.450 --> 00:10:03.772
different populations, which is
a common task for data scientists, and

161
00:10:03.772 --> 00:10:07.239
we'll followup some of this work in
the second course in this series.

162
00:10:09.320 --> 00:10:11.731
This lecture also
completes the lectures for

163
00:10:11.731 --> 00:10:15.735
the first course in the Applied Data
Science with Python Specialization.

164
00:10:15.735 --> 00:10:18.350
We've covered the basics
of Python programming,

165
00:10:18.350 --> 00:10:22.199
some more advanced features like maps,
lambdas and list comprehensions.

166
00:10:22.199 --> 00:10:26.827
How to read and manipulate data using
the pandas library, including querying,

167
00:10:26.827 --> 00:10:31.538
joining, grouping and processing of DataFrames
and the creation of pivot tables.

168
00:10:31.538 --> 00:10:35.398
And now we've talked a little bit
about statistics in Python and

169
00:10:35.398 --> 00:10:38.274
dug deeper into the NumPy and
SciPy toolkit.

170
00:10:38.274 --> 00:10:41.717
In the next course, we'll dig into
plotting and charting of data.

171
00:10:41.717 --> 00:10:45.056
Dealing a bit more with statistics and
how we present data to others, and

172
00:10:45.056 --> 00:10:48.160
how we build a compelling story for
the data we have.

173
00:10:48.160 --> 00:10:48.690
I'll see you then.
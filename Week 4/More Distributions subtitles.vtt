WEBVTT

1
00:00:08.895 --> 00:00:13.571
Many of the distributions you use in data
science are not discrete binomial, and

2
00:00:13.571 --> 00:00:17.631
instead are continuous, where the value
of the given observation isn't

3
00:00:17.631 --> 00:00:21.852
a category like heads or tails, but
can be represented as a real number.

4
00:00:21.852 --> 00:00:25.959
It's common to then graph these
distributions when talking about them,

5
00:00:25.959 --> 00:00:28.741
where the x axis is the value
of the observation and

6
00:00:28.741 --> 00:00:33.000
the y axis represents the probability
that a given observation will occur.

7
00:00:34.030 --> 00:00:38.040
If all numbers are equally likely to
be drawn when you sample from it,

8
00:00:38.040 --> 00:00:41.050
this should be graphed as
a flat horizontal line.

9
00:00:41.050 --> 00:00:44.519
And this flat line is actually
called the uniform distribution.

10
00:00:45.670 --> 00:00:48.910
There are few other distributions
that get a lot more interesting.

11
00:00:48.910 --> 00:00:52.910
Let's take the normal distribution which
is also called the Gaussian Distribution or

12
00:00:52.910 --> 00:00:54.470
sometimes, a Bell Curve.

13
00:00:55.570 --> 00:00:58.490
This distribution looks like
a hump, where the number which has

14
00:00:58.490 --> 00:01:01.650
the highest probability of
being drawn is a zero, and

15
00:01:01.650 --> 00:01:05.030
there are two decreasing curves
on either side of the X axis.

16
00:01:06.250 --> 00:01:09.930
One of the properties of this
distribution is that the mean is zero,

17
00:01:09.930 --> 00:01:12.360
and that the two curves on
either side are symmetric.

18
00:01:12.360 --> 00:01:16.540
I want to introduce you to
the term expected value.

19
00:01:16.540 --> 00:01:20.210
I think that most of us are familiar with
the mean is the sum of all the values

20
00:01:20.210 --> 00:01:22.790
divided by the total number of values.

21
00:01:22.790 --> 00:01:26.166
Calculating a mean value is
a computational process, and

22
00:01:26.166 --> 00:01:29.418
it takes place by looking at
samples from distribution.

23
00:01:29.418 --> 00:01:33.950
For instance rolling a die
three times might give you the numbers 1,

24
00:01:33.950 --> 00:01:37.266
2 and 6, the mean value is then 4.5.

25
00:01:37.266 --> 00:01:42.701
The expected value is the probability
from the underlying distribution. It is

26
00:01:42.701 --> 00:01:47.910
what would be the mean of a die roll
if we did an infinite number of rolls.

27
00:01:47.910 --> 00:01:53.660
The result is 3.5 since each face of
the die is equally likely to appear.

28
00:01:53.660 --> 00:01:56.089
Thus the expected value is 3.5,

29
00:01:56.089 --> 00:02:00.620
while the mean value depends upon
the samples that we've taken, and

30
00:02:00.620 --> 00:02:05.260
converges to the expected value given
a sufficiently large sample set.

31
00:02:05.260 --> 00:02:08.080
We'll talk more about expected
values in course three.

32
00:02:08.080 --> 00:02:10.600
A second property

33
00:02:10.600 --> 00:02:15.490
is that the variance of the distribution
can be described in a certain way.

34
00:02:15.490 --> 00:02:22.152
The variance is a measure of how badly values
of samples are spread out from the mean.

35
00:02:22.152 --> 00:02:26.376
Let's get a little bit more formal
about five different characteristics of

36
00:02:26.376 --> 00:02:27.368
distributions.

37
00:02:27.368 --> 00:02:31.453
First, we can talk about
the distributionâ€™s central tendency,

38
00:02:31.453 --> 00:02:36.510
and the measures we would use for
this are mode, median, or mean.

39
00:02:36.510 --> 00:02:40.360
This characteristic is really about
where the bulk of probability

40
00:02:40.360 --> 00:02:41.729
is in the distribution..

41
00:02:42.900 --> 00:02:46.030
We can also talk about
the variability in the distribution.

42
00:02:46.030 --> 00:02:49.070
There are a couple of ways
we can speak of this.

43
00:02:49.070 --> 00:02:52.660
The standard deviation is one,
the interquartile range is another.

44
00:02:52.660 --> 00:02:57.500
The standard deviation is simply
a measure of how different each item,

45
00:02:57.500 --> 00:02:59.530
in our sample is from the mean.

46
00:03:00.720 --> 00:03:02.520
Here's the formula for standard deviation.

47
00:03:04.040 --> 00:03:07.810
It might look a little more
intimidating than it actually is.

48
00:03:07.810 --> 00:03:10.296
Let's just walk through how
we would write this up.

49
00:03:10.296 --> 00:03:14.806
Let's draw 1,000 samples from
a normal distribution with

50
00:03:14.806 --> 00:03:19.156
an expected value of 0.75 and
a standard deviation of 1.

51
00:03:19.156 --> 00:03:23.840
Then we calculate the actual
mean using NumPy's mean feature.

52
00:03:23.840 --> 00:03:28.535
The part inside the summation
says xi - xbar.

53
00:03:28.535 --> 00:03:32.620
xi is the current item in the list and
xbar is the mean.

54
00:03:32.620 --> 00:03:35.660
So we calculate the difference,
then we square the result,

55
00:03:35.660 --> 00:03:37.090
then we sum all of these.

56
00:03:38.130 --> 00:03:42.250
This might be a reasonable place to use
a map and apply a lambda to calculate

57
00:03:42.250 --> 00:03:45.480
the differences between the mean and
the measured value,

58
00:03:45.480 --> 00:03:49.620
then to convert this back to a list,
so NumPy can use it.

59
00:03:49.620 --> 00:03:55.140
Now we just have to square each value, sum
them together, and take the square root.

60
00:03:55.140 --> 00:03:58.087
So that's the size of
our standard deviation.

61
00:03:58.087 --> 00:04:01.199
It covers roughly 68% of
the area around the mean,

62
00:04:01.199 --> 00:04:04.560
split evenly around the side of the mean.

63
00:04:04.560 --> 00:04:08.057
Now we don't normally have to
do all this work ourselves, but

64
00:04:08.057 --> 00:04:11.556
I wanted to show you how you can
sample from the distribution,

65
00:04:11.556 --> 00:04:16.346
create a precise programmatic description
of a formula, and apply it to your data.

66
00:04:16.346 --> 00:04:21.121
But for standard deviation, which is just
one particular measure of variability,

67
00:04:21.121 --> 00:04:24.763
NumPy has a built-in function
that you can apply, called std.

68
00:04:24.763 --> 00:04:28.805
There's a couple more measures of
distributions that are interesting to

69
00:04:28.805 --> 00:04:30.270
talk about.

70
00:04:30.270 --> 00:04:33.590
One of these is the shape of
the tales of the distribution and

71
00:04:33.590 --> 00:04:36.070
this is called the kurtosis.

72
00:04:36.070 --> 00:04:41.050
We can measure the kurtosis using the
statistics functions in the SciPy package.

73
00:04:41.050 --> 00:04:43.720
A negative value means
the curve is slightly more flat

74
00:04:43.720 --> 00:04:47.460
than a normal distribution, and
a positive value means the curve is

75
00:04:47.460 --> 00:04:50.760
slightly more peaky than
a normal distribution.

76
00:04:50.760 --> 00:04:55.160
Remember that we aren't measuring the
kurtosis of the distribution per se, but

77
00:04:55.160 --> 00:04:59.280
of the thousand values which we
sampled out of the distribution.

78
00:04:59.280 --> 00:05:01.600
This is a sublet but
important distinction.

79
00:05:03.340 --> 00:05:05.890
We could also move out of
the normal distributions and

80
00:05:05.890 --> 00:05:08.670
push the peak of the curve one way or
the other.

81
00:05:08.670 --> 00:05:09.740
And this is called the skew.

82
00:05:10.760 --> 00:05:14.736
If we test our current sample data,
we see that there isn't much of a skew.

83
00:05:14.736 --> 00:05:19.619
Let's switch distributions and take a look
at a distribution called the Chi Squared

84
00:05:19.619 --> 00:05:23.419
distribution, which is also quite
commonly used in statistics.

85
00:05:23.419 --> 00:05:28.360
The Chi Squared Distribution has only one
parameter called the degrees of freedom.

86
00:05:28.360 --> 00:05:32.740
The degrees of freedom is closely related
to the number of samples that you take

87
00:05:32.740 --> 00:05:34.640
from a normal population,

88
00:05:34.640 --> 00:05:37.370
and it's important for significance testing.

89
00:05:37.370 --> 00:05:41.870
But what I would like you to observe, is
that as the degrees of freedom increases,

90
00:05:41.870 --> 00:05:45.760
the shape of the Chi Squared
distribution changes.

91
00:05:45.760 --> 00:05:50.570
In particular, the skew to the left
begins to move towards the center.

92
00:05:50.570 --> 00:05:52.580
We can observe this through simulation.

93
00:05:53.630 --> 00:05:57.540
First we'll sample 1,000 values from
a Chi Squared distribution with degrees of

94
00:05:57.540 --> 00:05:59.010
freedom 2.

95
00:05:59.010 --> 00:06:01.961
Now we can see that
the skew is quite large.

96
00:06:01.961 --> 00:06:05.604
Now if we resample changing
degrees of freedom to 5.

97
00:06:08.687 --> 00:06:10.960
We see that the skew has
decreased significantly.

98
00:06:12.030 --> 00:06:15.560
We can actually plot this
right in the Jupyter notebook.

99
00:06:15.560 --> 00:06:18.880
I'm not going to talk much about
the library we're using here for plotting,

100
00:06:18.880 --> 00:06:20.810
because that's the topic
of the next course.

101
00:06:21.890 --> 00:06:25.580
But you can see a histogram with our plot
with the two degrees of freedom is skewed

102
00:06:25.580 --> 00:06:27.140
much further to the left,

103
00:06:27.140 --> 00:06:30.430
while our plot with the five degrees
of freedom is not as highly skewed.

104
00:06:31.490 --> 00:06:34.500
I would encourage you, as always,
to play with this notebook and

105
00:06:34.500 --> 00:06:35.820
change the parameters and

106
00:06:35.820 --> 00:06:39.819
see how the degrees of freedom
changes the skew of the distribution.

107
00:06:40.960 --> 00:06:45.340
The last aspect of distributions that
I want to talk about is the modality.

108
00:06:45.340 --> 00:06:50.460
So far, all of the distributions I've
shown have a single high point, a peak.

109
00:06:50.460 --> 00:06:52.930
But what if we have multiple peaks?

110
00:06:52.930 --> 00:06:56.925
This distribution has two high points,
so we call it bimodal.

111
00:06:56.925 --> 00:07:01.698
These are really interesting distributions
and happen regularly in data mining.

112
00:07:01.698 --> 00:07:04.770
We're going to talk a bit more
about them in course three,

113
00:07:04.770 --> 00:07:08.110
but a useful insight is that we
can actually model these using

114
00:07:08.110 --> 00:07:11.720
two normal distributions
with different parameters.

115
00:07:11.720 --> 00:07:14.210
These are called
Gaussian Mixture Models and

116
00:07:14.210 --> 00:07:16.840
are particularly useful
when clustering data.

117
00:07:18.310 --> 00:07:21.470
Well, this has been a long lecture but
I think it can be tough to chop

118
00:07:21.470 --> 00:07:26.000
up a discussion of distributions and
still get the main points across.

119
00:07:26.000 --> 00:07:30.770
Remember that a distribution is just
a shape that describes the probability

120
00:07:30.770 --> 00:07:34.730
of a value being pulled when
we sample a population.

121
00:07:34.730 --> 00:07:39.690
And NumPy and SciPy each have a number
of different distributions built in for

122
00:07:39.690 --> 00:07:41.060
us to be able to sample from.

123
00:07:42.510 --> 00:07:45.080
The last point I want to leave
you with here is a reference,

124
00:07:45.080 --> 00:07:49.210
if you find this way of exploring
statistics interesting.

125
00:07:49.210 --> 00:07:53.810
Alan Downey wrote a nice book called
Think Stats by the publisher O'Reilly.

126
00:07:53.810 --> 00:07:57.230
I think he does a really nice job of
teaching how to think about statistics

127
00:07:57.230 --> 00:07:59.520
from a programming perspective.

128
00:07:59.520 --> 00:08:03.360
One where you write the functions
behind the statistical methods.

129
00:08:03.360 --> 00:08:04.920
It's not really a reference book, but

130
00:08:04.920 --> 00:08:08.240
it's an interesting way to approach
learning the fundamentals of statistics.

131
00:08:09.470 --> 00:08:13.730
Allen even has a free copy of this book
available on his website in PDF format

132
00:08:13.730 --> 00:08:16.330
and, of course,
all of the code is done in Python.